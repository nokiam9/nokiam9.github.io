---
title: 大模型学习笔记之三：损失函数
date: 2024-07-07 20:59:08
tags:
---

在机器学习中，损失函数是代价函数的一部分，而代价函数则是目标函数的一种类型。

- 损失函数（Loss function）：用于定义**单个训练样本**与真实值之间的误差；
- 代价函数（Cost function）：用于定义**单个批次**，或整个训练集样本与真实值之间的误差；
- 目标函数（Objective function）：泛指任意可以被优化的函数。

损失函数是用于衡量模型所作出的预测离真实值（Ground Truth）之间的偏离程度。通常，我们都会最小化目标函数，最常用的算法便是“梯度下降法”（Gradient Descent）。俗话说，任何事情必然有它的两面性，因此，并没有一种万能的损失函数能够适用于所有的机器学习任务，所以在这里我们需要知道每一种损失函数的优点和局限性，才能更好的利用它们去解决实际的问题。

损失函数大致可分为两种：回归损失（针对连续型变量）和分类损失（针对离散型变量）。





## 一、概述

在深度学习广为使用的今天，我们可以在脑海里清晰的知道，一个模型想要达到很好的效果需要学习（=训练）。一个好的训练离不开优质的负反馈，损失函数（Loss Functions）就是模型的负反馈。
简化地说，损失函数可以定义为具有两个参数的函数：一是预测输出（Predicted Output），二是实际输出（True Output）。

![Group](demo.png)

神经网络可以执行多种任务，从预测连续值（如每月支出）到对离散类别（如猫和狗）进行分类。每个不同的任务将需要不同的损失类型，因为输出格式将不同。具体任务将定义不同的损失函数。

> 交叉熵,KL散度,极大似然估计这几种求解模型参数的方法，最终的结果都是等价的，由于交叉熵形式比较接近最终损失函数形式，人们默认交叉熵就是softmax分类器的损失函数。但是要注意，这三者的等价的关键在于标签只有一类是正确分类，这符合分类问题的定义。但如果你考虑的问题不是分类问题，或者标签是非平凡的（并不只有一类为1，其它全为0）,再使用交叉熵就不合理了。
> KL散度，全称为 Kullback-Leibler 散度（Kullback-Leibler divergence），也被称为相对熵，是一种衡量两个概率分布差异的度量。

## 二、面向分类的损失函数

对于二分类问题，y∈{−1,+1}，损失函数常表示为关于 $yf(x)$ 的单调递减形式。$yf(x)$ 被称为 margin ，最小化损失函数也可以看作是最大化 margin 的过程，任何合格的分类损失函数都应该对 $margin<0$ 的样本施以较大的惩罚。

从上面可以看出交叉熵,KL散度,极大似然估计这几种求解模型参数的方法，最终的结果都是等价的，由于交叉熵形式比较接近最终损失函数形式，人们默认交叉熵就是softmax分类器的损失函数。但是要注意，这三者的等价的关键在于标签只有一类是正确分类，这符合分类问题的定义。但如果你考虑的问题不是分类问题，或者标签是非平凡的（并不只有一类为1，其它全为0）,再使用交叉熵就不合理了。
logistic 回归只是 softmax 应用在二分类问题上的特殊情况，其本质都是一样的，这里不再赘述。

总的来说，损失函数的形式千变万化，但追究溯源还是万变不离其宗。其本质便是给出一个能较全面合理的描述两个特征或集合之间的相似性度量或距离度量，针对某些特定的情况，如类别不平衡等，给予适当的惩罚因子进行权重的加减。大多数的损失都是基于最原始的损失一步步改进的，或提出更一般的形式，或提出更加具体实例化的形式。

### 1. Cross Entropy Loss（交叉熵的损失函数）

### 2. Focal Loss

Focal loss 损失函数是为了解决 one-stage 目标检测中**正负样本极度不平衡**的问题，是一个密集目标检测的损失函数。在训练深层神经网络解决目标检测和分类问题时，这是最常见的选择之一。

焦点损失，出自何凯明的《Focal Loss for Dense Object Detection》[4]，出发点是解决目标检测领域中one-stage算法如YOLO系列算法准确率不高的问题。作者认为样本的类别不均衡（比如前景和背景）是导致这个问题的主要原因。比如在很多输入图片中，我们利用网格去划分小窗口，大多数的窗口是不包含目标的。如此一来，如果我们直接运用原始的交叉熵损失，那么负样本所占比例会非常大，主导梯度的优化方向，即网络会偏向于将前景预测为背景。即使我们可以使用OHEM（在线困难样本挖掘）算法来处理不均衡的问题，虽然其增加了误分类样本的权重，但也容易忽略掉易分类样本。而Focal loss则是聚焦于训练一个困难样本的稀疏集，通过直接在标准的交叉熵损失基础上做改进，引进了两个惩罚因子，来减少易分类样本的权重，使得模型在训练过程中更专注于困难样本。其基本定义如下：

实验中，作者取（α=0.25，γ=0.2）的效果最好，具体还需要根据任务的情况调整。由此可见，应用Focal-loss也会引入多了两个超参数需要调整，而一般来说很需要经验才能调好。

### 3. Poly Loss

可以将损失函数视为多项式函数的线性组合，并通过泰勒展开来近似函数。在多项式展开下，Focal Loss是多项式系数相对于Cross-entropy loss的水平位移。如果垂直修改多项式系数，则得到了Polyloss的计算公式：

Polyloss是Cross-entropy loss损失函数的一种广义形式。

### 4. Hinge Loss

Hinge loss损失函数通常适用于二分类的场景中，可以用来解决间隔最大化的问题，常应用于著名的SVM算法中。
Hinge 损失函数是一个凸函数，擅长“最大余量”分类，因此许多机器学习中常用的凸优化器都可以利用它。

Hinge 损失函数将与分类边界之间的差值或距离纳入成本计算。即使新的观察结果被正确分类，如果决策边界的差距不够大，它们也会受到惩罚，损失呈线性增加。

## 三、面向回归的损失函数

回归问题中 y 和 f(x) 都是实数，因此直接使用**残差**（ $y−f(x)$）来度量二者的不一致程度。残差 (的绝对值) 越大，则损失函数越大，学习出来的模型效果就越差（这里不考虑正则化问题）。

### 1. MAE（Mean Averange Error，平均绝对误差）

平均绝对误差（MAE）是另一种常用的回归损失函数，它是目标值与预测值之差绝对值和的均值，表示了预测值的平均误差幅度，而不需要考虑误差的方向，也称为 **L1 Loss 损失函数**（注：平均偏差误差 MBE 则是考虑的方向的误差，是残差的和），范围是 0 到 $\infty$，其公式如下所示：
$$ MAE = \frac{1}{N} \sum_{i=1}^{N} \vert y_i-f(x_i) \vert $$

### 2. MSE（Mean Square Error，均方误差）

均方误差（Mean Square Error,MSE）是回归损失函数中最常用的误差，它是预测值f(x)与目标值y之间差值平方和的均值，也称为 **L2 Loss 损失函数**，其公式如下所示：
$$ MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i-f(x_i))^2 $$

### 3. Huber Loss

MSE 和 MAE 各有优点和缺点，Huber 提出了一个综合二者特点的改进方案，公式如下：
$$ Loss(y,f(x)) =
\begin{cases}
    \frac {1}{2} (y-f(x))^2, & \vert y-f(x) \vert < \delta \\\\
    \delta \vert y-f(x) \vert - \frac {1}{2} \delta ^2, & \vert y-f(x) \vert \geqslant \delta
\end{cases}
$$

Huber Loss 包含了一个超参数$\delta$，$\delta$ 的大小决定了 Huber Loss 对 MSE 和 MAE 的侧重性，使其同时具备两者的优点，减小了对离群点的敏感度问题，还实现了处处可导的功能。

- 当$\vert y−f(x) \vert < \delta$ 时，变为 MSE，梯度逐渐减小，保证模型更精确地得到全局最优解
- 当$\vert y−f(x) \vert \geqslant \delta$时，则变成类似于 MAE，梯度一直近似为 $\delta$ ，保证了模型的快速收敛
  
通常来说，超参数 $\delta$ 可以通过交叉验证选取最佳值。下面分别取 $\delta$ = 3、$\delta$ = 1.5，绘制相应的 Huber Loss，如下图所示：

![回归类](regression.png)

### 4. Smooth L1（平滑之后的L1）

根据 [PyTorch SmoothL1Loss 函数](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#smoothl1loss)的定义，其公式为：
$$ Loss(y,f(x)) =
\begin{cases}
    \frac{1}{2\delta}(y-f(x))^2, & \vert y - f(x) \vert < \delta  \\\\
    \vert y-f(x) \vert - \frac {1}{2}\delta, & \vert y - f(x) \vert \geqslant \delta
\end{cases}
$$

Smooth L1 Loss 与 Huber Loss 的效果差不多，只是构造函数有所区别。

### 小结

- 从梯度的求解以及收敛上，MSE 是优于 MAE 的。MSE 处处可导，而且梯度值也是动态变化的，能够快速的收敛；而 MAE 在零点处不可导，且其梯度保持不变。对于很小的损失值其梯度也很大，在深度学习中，就需要使用变化的学习率，在损失值很小时降低学习率。
- 对离群（异常）值的处理上，MAE 要明显好于 MSE 。如果离群点（异常值）需要被检测出来，则可以选择 MSE 作为损失函数；如果离群点只是当做受损的数据处理，则可以选择 MAE 作为损失函数。

## 三、结论

![Group](family.jpg)

> 注意！此图的 Smooth L1 Loss 函数定义有错误，PyTorch 的定义应该是准确的。


---

## 附录一：关于熵的思考

你可能还听说过一个叫做二八定律的东西，20%的人掌握着社会80%的财富，把这种离散的模型迁移到连续的情况，这就是帕累托分布（这是一个厚尾模型，它的u均值在采样无限时可以趋于无限大）从如下的图像可以看到，采样时大部分样本都聚集在前面，但采样足够多时，总有x非常大的样本来影响采样数据整体的分布，这个概率分布的熵值就很低。

与帕累托分布相比，正态分布的熵值就非常大，这也对应了事物演化的开始和结局；有一种说法，宇宙诞生时是符合帕累托分布的，它的熵值很低，也具有很大的活力；宇宙热寂时，它的熵值很大，符合正态分布，也就是说事物的发展过程就是从幂律分布到正态分布

还拿二八定律举例，我们目前社会的财富比例符合二八定律，大部分财富在少部分人手中，但它最终必将演化到正态分布，大部分财富掌握在大部分人手中，这时社会形态就从资本主义演化到共产主义，我们又从信息论的理论证明了马克思主义的正确性

---

## 参考文献
  
- [神经网络中的损失函数](https://cloud.tencent.com/developer/article/2322229)
- [一文看尽深度学习中的各种损失函数](https://www.cvmart.net/community/detail/4879)
- [MSE, MAE, Huber loss详解](https://blog.csdn.net/nefetaria/article/details/111238515)
- [深入浅出PyTorch - 损失函数](https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B8%89%E7%AB%A0/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html)
- [交叉熵损失函数（cross-entropy loss function）原理及Pytorch代码简介](https://blog.csdn.net/chao_shine/article/details/89925762)

### 视频讲解

- [“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”](https://www.bilibili.com/video/BV15V411W7VB/?spm_id_from=333.788&vd_source=735a6376f6214c7b974a1074096ba0fa)
- [“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法”](https://www.bilibili.com/video/BV1Y64y1Q7hi/?spm_id_from=333.788&vd_source=735a6376f6214c7b974a1074096ba0fa)
- [常见的损失函数](https://www.bilibili.com/video/BV1p5411c7nZ/?spm_id_from=333.337.search-card.all.click&vd_source=735a6376f6214c7b974a1074096ba0fa)


