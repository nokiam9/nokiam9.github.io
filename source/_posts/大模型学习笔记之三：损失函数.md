---
title: 大模型学习笔记之三：损失函数
date: 2024-07-07 20:59:08
tags:
---

在机器学习中，损失函数是代价函数的一部分，而代价函数则是目标函数的一种类型。

- 损失函数（Loss function）：用于定义**单个训练样本**与真实值之间的误差；
- 代价函数（Cost function）：用于定义**单个批次**，或整个训练集样本与真实值之间的误差；
- 目标函数（Objective function）：泛指任意可以被优化的函数。

损失函数是用于衡量模型所作出的预测离真实值（Ground Truth）之间的偏离程度。通常，我们都会最小化目标函数，最常用的算法便是“梯度下降法”（Gradient Descent）。俗话说，任何事情必然有它的两面性，因此，并没有一种万能的损失函数能够适用于所有的机器学习任务，所以在这里我们需要知道每一种损失函数的优点和局限性，才能更好的利用它们去解决实际的问题。

损失函数大致可分为两种：回归损失（针对连续型变量）和分类损失（针对离散型变量）。



## 一、信息论的基本原理

### 1. 信息量

信息量，是对包含的信息多少/大小的一个度量，也称为“自信息(self-information)”。通常采用如下定义：
$$I(x) = -log_2p(x)$$

对于**一个已经确定发生的事件**，其提供的信息量与其发生的概率是呈**负相关**的。举个例子，假定北京天气是晴天的概率是80%，下雨的概率是20%，那么：

- 如果天气预报说“今天是晴天”，提供的信息量就是 $-log_20.8=0.322$，说明这是一个大概率事件
- 如果天气预报说“今天是雨天”，提供的信息量就是 $-log_20.2=2.322$，说明这个一个小概率事件

一个100%概率发生的事件，其信息量就是 $-log_21=0$；“国足踢进了世界杯“这种非常小概率的事件，一定有很多曲折的内幕，其蕴含的信息量就非常大；最极端的，如果发生概率为 0，那么其信息量就是无穷大了！

### 2.信息熵（Information Entropy）

信息熵（Entropy）是接收到的每条信息所包含的信息量的平均值（期望），又被称为香农熵。
换句话说，事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望），就是这个分布产生的信息量的平均值，即为信息熵。
$$H(x) = - \sum_{i=1}^{n} p(x_i)log{p(x_i)}$$

还是以天气预报为例，考虑几种不同的概率分布：

- 如果晴天概率80%，雨天概率20%，信息熵为 $-(0.8 \times log_{2}0.8 + 0.2 \times log_{2}0.2) =0.7219$
- 如果晴天和雨天的概率都是50%，信息熵为 $-(0.5 \times log_{2}0.5 + 0.5 \times log_{2}0.5) = 1$
- 再搞复杂一点，如果有四种天气，分别是晴天、阴天、小雨和大雨，其发生概率都是25%，则信息熵为 $-(0.25 \times log_{2}0.25 + 0.25 \times log_{2}0.25 + 0.25 \times log_{2}0.25 + 0.25 \times log_{2}0.25 = 2$

![XXS](xxs0.png)

信息熵的现实意义，可以有多个角度的理解：

1. 对于一个给定的（预测）概率分布，信息熵是**在结果出来之前**，对可能产生的信息量的期望，代表着基于该预测条件下，对系统不确定性的评估数值，即：信息熵越高，系统的不确定性更高，而对于一个确定的信息，信息熵为零，信息量为零。
   一般来说，如果其概率分布为一个均匀分布，则信息熵最大。下图展示了一个二元信源的熵函数：
    ![XXS](xxs1.png)
    在上面的例子中，预测二的概率分布比预测一更平均，因此不确定性更强；预测三虽然也是平均分布，但天气的状态数量预测二更多，也说明其不确定性更强。
2. 在信息论中，信息熵代表着根据信息的概率分布对信息编码所需要的**最短**平均编码长度。
    ![alt text](xxs2.png)
    对于上面的预测三，如果 4 种天气的概率系统，信息熵为 2，意味着最少需要 2 位编码表示；
    对于上图的不均匀分布，最优编码是 0、10、110、111，此时平均编码长度为 1.75 位，与信息熵完全相等。

### 3. 联合熵（Joint Entropy）

联合熵是衡量一组变量的不确定性的指标，显然其与联合概率有关。数学定义为：
$$ H(X,Y) := -\sum_{i=1}^{n} \sum_{y=1}^{m} p(x_i,y_j) log{p(x_i,y_j)}$$

这个公式也比较容易理解，如果 $X$ 与 $Y$ 是独立的，根据概率论的知识可知，$P(x,y) = P(x)*P(y)$，则可得到 $H(X, Y) = H(X) + H(Y)$ , 可从两个变量推广到 N 个变量的情况。

> 联合熵的物理意义是：观察一个多个随机变量的随机系统获得的信息量。

### 4. 条件熵（Conditional Entropy）

在给定 $Y = \\{y_1, y_2, y_3 … y_n\\}$ 发生的前提下，求事件 $X = \\{x_1, x_2, x_3 … x_m\\}$ 的熵，称为 $X$ 的条件熵，用来衡量 $Y$ 发生的不确定性, 此时用 $H(X|Y)$ 来表示 $X$ 的条件。
与熵是一样的，条件熵越大，表示该事件的不确定性越大。数学定义是：
$$ H(X|Y) := -\sum_{i=1}^{n} \sum_{y=1}^{m} P(x_i,y_j) log \frac {1}{P(x_i|y_j)}$$

> 条件熵的物理意义就是：在得知某一确定信息的基础上获取另外一个信息时所获得的信息量。

### 5. 互信息（Mutual Information）

互信息（Mutual Information）是衡量已知一个变量时，另一个变量不确定性的减少程度。两个离散随机变量  X和 Y的互信息定义为
$$ I(X,Y) = \sum_{i=1}^{n} \sum_{j=1}^{m} p(x,y)log \frac {p(x,y)}{p(x)p(y)} $$

如果 X 和  Y 互相独立，即  X 和 Y 之间互相不提供任何信息，反之亦然，因此他们的互信息为 0。

对于两个随机变量 𝑋,𝑌，它们的互信息可以定义为 𝑋,𝑌 的联合分布和对立分布乘积的相对熵。


经过变形和计算可以得到互信息：

𝐼(𝑋;𝑌)=𝐻(𝑋)+𝐻(𝑌)−𝐻(𝑋,𝑌)
互信息的意义是衡量 𝑋 到 𝑌 的不确定性的减少程度，(衡量随机变量之间相互依赖程度的度量。)

另外互信息是对称的（symmetric），也就是𝐼(𝑋;𝑌)=𝐼(𝑌;𝑋)，所以互信息不能用于确定信息流的方向。

使用概率的加和规则和乘积规则,我们看到互信息和条件熵之间的关系为

$$I(X,Y) = H(X)−H(X|Y) = H(Y) - H(Y|X)$$

### 6. 小结

对于随机变量 𝑋,𝑌，它们的熵、联合熵、条件熵以及互信息之间的关系是：
![sum](sum0.png)

其中，左边的圆形区域表示随机变量 𝑋 的熵，右边的圆形区域表示随机变量 𝑌 的熵。
左边的 𝐻(𝑋∣𝑌) 区域表示在随机变量 𝑌 给定的条件下随机变量 𝑋 的条件熵；左边的 𝐻(𝑌∣𝑋) 区域表示在随机变量 𝑋 给定的条件下随机变量 𝑌 的条件熵。两个圆中间相交的部分表示随机变量 𝑋,𝑌 的互信息。两个圆构成的整体部分表示 𝑋,𝑌 的联合熵。

![sum](sum1.png)

## 二、数学原理的进阶篇

### 1. 相对熵（Relative Entropy）

相对熵是衡量两个不同分布之间的**距离**或者相似的一个指标。也称为 KL散度（Kullback-Leibler Divergence）。
假设 $P(x)$ 是目标分布，$Q(x)$ 是评价分布，衡量$Q(x)$与目标分布 $P(x)$ 之间的距离，就可以使用这个指标实现。公式是，
$$D_{KL}(p||q) = \sum_{i=1}^n p(x_i)log(\frac {p(x_i)}{q(x_i)})$$

相对熵是衡量两个不同分布之间的“距离”或者相似的一个指标。假设这两个分布是P(x)和Q(x)，其中P(x)是目标分布，Q(x)是评价分布，衡量Q(x)与目标分布P(x)之间的距离，就可以使用这个指标实现。公式是，
`相对熵 = 交叉熵 - 信息熵`

![KL](KL.png)

#### KL 散度的特性

1. 非对称性：$ D_{KL}(P \\| Q) \neq D_{KL}(Q \Vert P) $。即 KL 散度不是一个对称量
2. 非负性：$ D_{KL}(P || Q) \geqslant 0 $，当且仅当 $P(x) = Q(x) $ 时等号成立。根据吉布斯不等式可以证明

KL 散度 = H(P,Q) - H(P) = 交叉熵 - 信息熵

在模型的训练阶段，输入数据和标签常常已经确定，那么真实概率分布P(X)是确定，所以信息熵在这里是一个常量。
由于KL散度的值表示真实概率分布P(x)与预测概率分布Q(x)之间的差异，值越小表示两个分布之间的差异性越小，所以目标变成最小化KL散度。但是由于交叉熵等于KL散度加上一个常量，且公式相比KL散度更加容易，因此最小化KL散度与最小化交叉熵的效果是一致的，且优化交叉熵的计算量小于优化KL散度。


联合熵是衡量一组变量的不确定性的指标，显然其与联合概率有关。公式为，

### 2. 交叉熵(Cross Entropy)

交叉熵是在机器学习的分类任务里被广泛使用的目标函数(objective function)/损失函数(loss function)。上面提到了，理论上相对熵才是衡量两个分布是否相近的指标，而在实际中为什么使用的是交叉熵呢？通过下面的推导公式，可以得出结论

给定一个策略，交叉熵就是在该策略下猜中颜色所需要的问题的期望值。交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出成本的大小。交叉的字面意思在于：真实分布与非真实分布的交叉。给定一个方案，越优的策略，交叉熵越低，具有最低的交叉熵策略就是最优的策略。而在此时，交叉熵=信息熵。因此，我们通常需要最小化交叉熵，也间接证明了我们的算法所算出的非真实分布接近真实分布。交叉熵也叫对数似然


### 3. 小结


交叉熵：预测越准确，交叉熵越小；交叉熵只与真实标签的预测概率相关
$$ H(P,q) = - \sum_{i=1}^{n} p(x_i)logq(x_i) $$
二分类公式：$ H(P,Q) = -(plogq + (1-p)log(1-q))$



## 一、概述

在深度学习广为使用的今天，我们可以在脑海里清晰的知道，一个模型想要达到很好的效果需要学习（=训练）。一个好的训练离不开优质的负反馈，损失函数（Loss Functions）就是模型的负反馈。
简化地说，损失函数可以定义为具有两个参数的函数：一是预测输出（Predicted Output），二是实际输出（True Output）。

![Group](demo.png)

神经网络可以执行多种任务，从预测连续值（如每月支出）到对离散类别（如猫和狗）进行分类。每个不同的任务将需要不同的损失类型，因为输出格式将不同。具体任务将定义不同的损失函数。

> 交叉熵,KL散度,极大似然估计这几种求解模型参数的方法，最终的结果都是等价的，由于交叉熵形式比较接近最终损失函数形式，人们默认交叉熵就是softmax分类器的损失函数。但是要注意，这三者的等价的关键在于标签只有一类是正确分类，这符合分类问题的定义。但如果你考虑的问题不是分类问题，或者标签是非平凡的（并不只有一类为1，其它全为0）,再使用交叉熵就不合理了。
> KL散度，全称为 Kullback-Leibler 散度（Kullback-Leibler divergence），也被称为相对熵，是一种衡量两个概率分布差异的度量。

## 二、面向分类的损失函数

对于二分类问题，y∈{−1,+1}，损失函数常表示为关于 $yf(x)$ 的单调递减形式。$yf(x)$ 被称为 margin ，最小化损失函数也可以看作是最大化 margin 的过程，任何合格的分类损失函数都应该对 $margin<0$ 的样本施以较大的惩罚。

从上面可以看出交叉熵,KL散度,极大似然估计这几种求解模型参数的方法，最终的结果都是等价的，由于交叉熵形式比较接近最终损失函数形式，人们默认交叉熵就是softmax分类器的损失函数。但是要注意，这三者的等价的关键在于标签只有一类是正确分类，这符合分类问题的定义。但如果你考虑的问题不是分类问题，或者标签是非平凡的（并不只有一类为1，其它全为0）,再使用交叉熵就不合理了。
logistic 回归只是 softmax 应用在二分类问题上的特殊情况，其本质都是一样的，这里不再赘述。

### 1. Cross Entropy Loss（交叉熵的损失函数）

### 2. Focal Loss

### 3. Poly Loss

### 4. Hinge Loss

## 三、面向回归的损失函数

回归问题中 y 和 f(x) 都是实数，因此直接使用**残差**（ $y−f(x)$）来度量二者的不一致程度。残差 (的绝对值) 越大，则损失函数越大，学习出来的模型效果就越差（这里不考虑正则化问题）。

### 1. MAE（Mean Averange Error，平均绝对误差）

平均绝对误差（MAE）是另一种常用的回归损失函数，它是目标值与预测值之差绝对值和的均值，表示了预测值的平均误差幅度，而不需要考虑误差的方向，也称为 **L1 Loss 损失函数**（注：平均偏差误差 MBE 则是考虑的方向的误差，是残差的和），范围是 0 到 $\infty$，其公式如下所示：
$$ MAE = \frac{1}{N} \sum_{i=1}^{N} \vert y_i-f(x_i) \vert $$

### 2. MSE（Mean Square Error，均方误差）

均方误差（Mean Square Error,MSE）是回归损失函数中最常用的误差，它是预测值f(x)与目标值y之间差值平方和的均值，也称为 **L2 Loss 损失函数**，其公式如下所示：
$$ MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i-f(x_i))^2 $$

### 3. Huber Loss

MSE 和 MAE 各有优点和缺点，Huber 提出了一个综合二者特点的改进方案，公式如下：
$$ Loss(y,f(x)) =
\begin{cases}
    \frac {1}{2} (y-f(x))^2, & \vert y-f(x) \vert < \delta \\\\
    \delta \vert y-f(x) \vert - \frac {1}{2} \delta ^2, & \vert y-f(x) \vert \geqslant \delta
\end{cases}
$$

Huber Loss 包含了一个超参数$\delta$，$\delta$ 的大小决定了 Huber Loss 对 MSE 和 MAE 的侧重性，使其同时具备两者的优点，减小了对离群点的敏感度问题，还实现了处处可导的功能。

- 当$\vert y−f(x) \vert < \delta$ 时，变为 MSE，梯度逐渐减小，保证模型更精确地得到全局最优解
- 当$\vert y−f(x) \vert \geqslant \delta$时，则变成类似于 MAE，梯度一直近似为 $\delta$ ，保证了模型的快速收敛
  
通常来说，超参数 $\delta$ 可以通过交叉验证选取最佳值。下面分别取 $\delta$ = 3、$\delta$ = 1.5，绘制相应的 Huber Loss，如下图所示：

![回归类](regression.png)

### 4. Smooth L1（平滑之后的L1）

根据 [PyTorch SmoothL1Loss 函数](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#smoothl1loss)的定义，其公式为：
$$ Loss(y,f(x)) =
\begin{cases}
    \frac{1}{2\delta}(y-f(x))^2, & \vert y - f(x) \vert < \delta  \\\\
    \vert y-f(x) \vert - \frac {1}{2}\delta, & \vert y - f(x) \vert \geqslant \delta
\end{cases}
$$

Smooth L1 Loss 与 Huber Loss 的效果差不多，只是构造函数有所区别。

### 小结

- 从梯度的求解以及收敛上，MSE 是优于 MAE 的。MSE 处处可导，而且梯度值也是动态变化的，能够快速的收敛；而 MAE 在零点处不可导，且其梯度保持不变。对于很小的损失值其梯度也很大，在深度学习中，就需要使用变化的学习率，在损失值很小时降低学习率。
- 对离群（异常）值的处理上，MAE 要明显好于 MSE 。如果离群点（异常值）需要被检测出来，则可以选择 MSE 作为损失函数；如果离群点只是当做受损的数据处理，则可以选择 MAE 作为损失函数。

## 三、结论

![Group](family.jpg)

> 注意！此图的 Smooth L1 Loss 函数定义有错误，PyTorch 的定义应该是准确的。


---

## 附录一：关于熵的思考

你可能还听说过一个叫做二八定律的东西，20%的人掌握着社会80%的财富，把这种离散的模型迁移到连续的情况，这就是帕累托分布（这是一个厚尾模型，它的u均值在采样无限时可以趋于无限大）从如下的图像可以看到，采样时大部分样本都聚集在前面，但采样足够多时，总有x非常大的样本来影响采样数据整体的分布，这个概率分布的熵值就很低。

与帕累托分布相比，正态分布的熵值就非常大，这也对应了事物演化的开始和结局；有一种说法，宇宙诞生时是符合帕累托分布的，它的熵值很低，也具有很大的活力；宇宙热寂时，它的熵值很大，符合正态分布，也就是说事物的发展过程就是从幂律分布到正态分布

还拿二八定律举例，我们目前社会的财富比例符合二八定律，大部分财富在少部分人手中，但它最终必将演化到正态分布，大部分财富掌握在大部分人手中，这时社会形态就从资本主义演化到共产主义，我们又从信息论的理论证明了马克思主义的正确性

---

## 参考文献

- [熵 (Entropy)](https://medium.com/@eric_zhu/%E7%86%B5-entropy-8299ccce0cf2)
- [信息熵、交叉熵、KL-散度、联合熵、条件熵和互信息](https://gulico.github.io/2020/07/20/xinxilun/)
- [信息论基础（熵，互信息，交叉熵）- 老羊肖恩](https://www.jianshu.com/p/71bd778dfb5a)
- [机器学习里的信息论](https://www.cnblogs.com/ZihanZhang/p/16049215.html)
- [损失函数的解释](https://paulxiong.medium.com/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E8%A7%A3%E9%87%8A-c2b6f165c842)
- [神经网络中的损失函数](https://cloud.tencent.com/developer/article/2322229)
- [MSE, MAE, Huber loss详解](https://blog.csdn.net/nefetaria/article/details/111238515)
- [深入浅出PyTorch - 损失函数](https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B8%89%E7%AB%A0/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html)

### 视频讲解

- [“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”](https://www.bilibili.com/video/BV15V411W7VB/?spm_id_from=333.788&vd_source=735a6376f6214c7b974a1074096ba0fa)
- [“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法”](https://www.bilibili.com/video/BV1Y64y1Q7hi/?spm_id_from=333.788&vd_source=735a6376f6214c7b974a1074096ba0fa)
- [常见的损失函数](https://www.bilibili.com/video/BV1p5411c7nZ/?spm_id_from=333.337.search-card.all.click&vd_source=735a6376f6214c7b974a1074096ba0fa)
