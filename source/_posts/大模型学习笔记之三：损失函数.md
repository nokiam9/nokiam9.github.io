---
title: 大模型学习笔记之三：损失函数
date: 2024-07-07 20:59:08
tags:
---

## 一、概述

在深度学习广为使用的今天，我们可以在脑海里清晰的知道，一个模型想要达到很好的效果需要学习（=训练）。一个好的训练离不开优质的负反馈，损失函数（Loss Functions）就是模型的负反馈。
简化地说，损失函数可以定义为具有两个参数的函数：一是预测输出（Predicted Output），二是实际输出（True Output）。

![Group](demo.png)

神经网络可以执行多种任务，从预测连续值（如每月支出）到对离散类别（如猫和狗）进行分类。每个不同的任务将需要不同的损失类型，因为输出格式将不同。具体任务将定义不同的损失函数。

## 二、面向分类的损失函数

对于二分类问题，y∈{−1,+1}，损失函数常表示为关于 $yf(x)$ 的单调递减形式。$yf(x)$ 被称为 margin ，最小化损失函数也可以看作是最大化 margin 的过程，任何合格的分类损失函数都应该对 $margin<0$ 的样本施以较大的惩罚。

从上面可以看出交叉熵,KL散度,极大似然估计这几种求解模型参数的方法，最终的结果都是等价的，由于交叉熵形式比较接近最终损失函数形式，人们默认交叉熵就是softmax分类器的损失函数。但是要注意，这三者的等价的关键在于标签只有一类是正确分类，这符合分类问题的定义。但如果你考虑的问题不是分类问题，或者标签是非平凡的（并不只有一类为1，其它全为0）,再使用交叉熵就不合理了。
logistic 回归只是 softmax 应用在二分类问题上的特殊情况，其本质都是一样的，这里不再赘述。

### 1. Cross Entropy Loss（交叉熵的损失函数）

### 2. Focal Loss

### 3. Poly Loss

### 4. Hinge Loss

## 三、面向回归的损失函数

回归问题中 y 和 f(x) 都是实数，因此直接使用**残差**（ $y−f(x)$）来度量二者的不一致程度。残差 (的绝对值) 越大，则损失函数越大，学习出来的模型效果就越差（这里不考虑正则化问题）。

### 1. MAE（Mean Averange Error，平均绝对误差）

平均绝对误差（MAE）是另一种常用的回归损失函数，它是目标值与预测值之差绝对值和的均值，表示了预测值的平均误差幅度，而不需要考虑误差的方向，也称为 **L1 Loss 损失函数**（注：平均偏差误差 MBE 则是考虑的方向的误差，是残差的和），范围是 0 到 $\infty$，其公式如下所示：
$$ MAE = \frac{1}{N} \sum_{i=1}^{N} \vert y_i-f(x_i) \vert $$

### 2. MSE（Mean Square Error，均方误差）

均方误差（Mean Square Error,MSE）是回归损失函数中最常用的误差，它是预测值f(x)与目标值y之间差值平方和的均值，也称为 **L2 Loss 损失函数**，其公式如下所示：
$$ MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i-f(x_i))^2 $$

### 3. Huber Loss

MSE 和 MAE 各有优点和缺点，Huber 提出了一个综合二者特点的改进方案，公式如下：
$$ Loss(y,f(x)) =
\begin{cases}
    \frac {1}{2} (y-f(x))^2, & \vert y-f(x) \vert < \delta \\\\
    \delta \vert y-f(x) \vert - \frac {1}{2} \delta ^2, & \vert y-f(x) \vert \geqslant \delta
\end{cases}
$$

Huber Loss 包含了一个超参数$\delta$，$\delta$ 的大小决定了 Huber Loss 对 MSE 和 MAE 的侧重性，使其同时具备两者的优点，减小了对离群点的敏感度问题，还实现了处处可导的功能。

- 当$\vert y−f(x) \vert < \delta$ 时，变为 MSE，梯度逐渐减小，保证模型更精确地得到全局最优解
- 当$\vert y−f(x) \vert \geqslant \delta$时，则变成类似于 MAE，梯度一直近似为 $\delta$ ，保证了模型的快速收敛
  
通常来说，超参数 $\delta$ 可以通过交叉验证选取最佳值。下面分别取 $\delta$ = 3、$\delta$ = 1.5，绘制相应的 Huber Loss，如下图所示：

![回归类](regression.png)

### 4. Smooth L1（平滑之后的L1）

根据 [PyTorch SmoothL1Loss 函数](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#smoothl1loss)的定义，其公式为：
$$ Loss(y,f(x)) =
\begin{cases}
    \frac{1}{2\delta}(y-f(x))^2, & \vert y - f(x) \vert < \delta  \\\\
    \vert y-f(x) \vert - \frac {1}{2}\delta, & \vert y - f(x) \vert \geqslant \delta
\end{cases}
$$

Smooth L1 Loss 与 Huber Loss 的效果差不多，只是构造函数有所区别。

### 小结

- 从梯度的求解以及收敛上，MSE 是优于 MAE 的。MSE 处处可导，而且梯度值也是动态变化的，能够快速的收敛；而 MAE 在零点处不可导，且其梯度保持不变。对于很小的损失值其梯度也很大，在深度学习中，就需要使用变化的学习率，在损失值很小时降低学习率。
- 对离群（异常）值的处理上，MAE 要明显好于 MSE 。如果离群点（异常值）需要被检测出来，则可以选择 MSE 作为损失函数；如果离群点只是当做受损的数据处理，则可以选择 MAE 作为损失函数。

## 三、结论

![Group](family.jpg)

> 注意！此图的 Smooth L1 Loss 函数定义有错误，PyTorch 的定义应该是准确的。

---

## 参考文献

- [损失函数的解释](https://paulxiong.medium.com/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E8%A7%A3%E9%87%8A-c2b6f165c842)
- [神经网络中的损失函数](https://cloud.tencent.com/developer/article/2322229)
- [MSE, MAE, Huber loss详解](https://blog.csdn.net/nefetaria/article/details/111238515)
- [深入浅出PyTorch - 损失函数](https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B8%89%E7%AB%A0/3.6%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html)

### 视频讲解

- [“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”](https://www.bilibili.com/video/BV15V411W7VB/?spm_id_from=333.788&vd_source=735a6376f6214c7b974a1074096ba0fa)
- [“损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法”](https://www.bilibili.com/video/BV1Y64y1Q7hi/?spm_id_from=333.788&vd_source=735a6376f6214c7b974a1074096ba0fa)
- [常见的损失函数](https://www.bilibili.com/video/BV1p5411c7nZ/?spm_id_from=333.337.search-card.all.click&vd_source=735a6376f6214c7b974a1074096ba0fa)
