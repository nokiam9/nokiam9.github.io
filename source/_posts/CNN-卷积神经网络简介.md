---
title: CNN 卷积神经网络简介
date: 2024-05-26 20:51:18
tags:
---

## 一、基础知识

　　卷积神经网络（Convolutional Neural Network，简称CNN），是一种前馈神经网络，人工神经元可以响应周围单元，可以进行大型图像处理。卷积神经网络包括卷积层和池化层。 
卷积神经网络是受到生物思考方式启发的MLPs（多层感知器），它有着不同的类别层次，并且各层的工作方式和作用也不同。这里提供一个较好的CNN教程（http://cs231n.github.io/convolutional-networks/）。文章中详细介绍了CNN的计算方式和数据的流动过程，这里只做简单的介绍。

卷积神经网络是人工神经网络的一种，已成为当前语音分析和图像识别领域的研究热点。它的权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。该优点在网络的输入是多维图像时表现的更为明显，使图像可以直接作为网络的输入，避免了传统识别算法中复杂的特征提取和数据重建过程。卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。

CNNs是受早期的延时神经网络（TDNN）的影响。延时神经网络通过在时间维度上共享权值降低学习复杂度，适用于语音和时间序列信号的处理。

CNNs是第一个真正成功训练多层网络结构的学习算法。它利用空间关系减少需要学习的参数数目以提高一般前向BP算法的训练性能。CNNs作为一个深度学习架构提出是为了最小化数据的预处理要求。在CNN中，图像的一小部分（局部感受区域）作为层级结构的最低层的输入，信息再依次传输到不同的层，每层通过一个数字滤波器去获得观测数据的最显著的特征。这个方法能够获取对平移、缩放和旋转不变的观测数据的显著特征，因为图像的局部感受区域允许神经元或者处理单元可以访问到最基础的特征，例如定向边缘或者角点。

在卷积神经网络中,感受野(Receptive Field)是指特征图上的某个点能看到的输入图像的区域,即特征图上的点是由输入图像中感受野大小区域的计算得到的。
神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征；相反，值越小则表示其所包含的特征越趋向局部和细节。因此感受野的值可以用来大致判断每一层的抽象层次。

## 二、整体框架

CNN 的基本结构由输入层、卷积层、取样层、全连接层及输出层构成。
卷积层和取样层一般会取若干个，采用卷积层和取样层交替设置，即一个卷积层连接一个取样层，取样层后再连接一个卷积层，依此类推。由于卷积层中输出特征面的每个神经元与其输入进行局部连接，并通过对应的连接权值与局部输入进行加权求和再加上偏置值，得到该神经元输入值，该过程等同于卷积过程，卷积神经网络也由此而得名。

![car](car.jpg)

### 1. 输入层

与传统神经网络/机器学习一样，模型需要输入的进行预处理操作，常见的输入层中对图像预处理方式有：

- 去均值：把输入数据各个维度都中心化到0，所有样本求和求平均，然后用所有的样本减去这个均值样本就是去均值。
- 归一化：数据幅度归一化到同样的范围，对于每个特征而言，范围最好是 [-1, 1]
- PCA/SVD降维等：用PCA降维，让每个维度的相关度取消，特征和特征之间是相互独立的。白化是对数据每个特征轴上的幅度

### 2. 卷积层 - Convolutional Layer

### 3. 激励层

所谓激励，实际上是对卷积层的输出结果做一次非线性映射。 
如果不用激励函数（其实就相当于激励函数是f(x)=x），这种情况下，每一层的输出都是上一层输入的线性函数。容易得出，无论有多少神经网络层，输出都是输入的线性组合，与没有隐层的效果是一样的，这就是最原始的感知机了。 
常用的激励函数有：

Sigmoid函数
Tanh函数
ReLU
Leaky ReLU
ELU
Maxout

激励层建议：

- 首先ReLU，因为迭代速度快，但是有可能效果不加。
- 如果ReLU失效的情况下，考虑使用Leaky ReLU或者Maxout，此时一般情况都可以解决。
- Tanh函数在文本和音频处理有比较好的效果。

### 4. 取样层 - Pooling Layer

　　池化（Pooling）：也称为欠采样或下采样。主要用于特征降维，压缩数据和参数的数量，减小过拟合，同时提高模型的容错性。主要有：

Max Pooling：最大池化
Average Pooling：平均池化 
　　Max Pooling：选取最大的，我们定义一个空间邻域（比如，2*2的窗口），并从窗口内的修正特征图中取出最大的元素，最大池化被证明效果更好一些。

　　Average Pooling：平均的，我们定义一个空间邻域（比如，2*2的窗口），并从窗口内的修正特征图中算出平均值。

### 5. 全连接层 - Fully-Connected Layer

经过前面若干次卷积+激励+池化后，终于来到了输出层，模型会将学到的一个高质量的特征图片全连接层。其实在全连接层之前，如果神经元数目过大，学习能力强，有可能出现过拟合。因此，可以引入dropout操作，来随机删除神经网络中的部分神经元，来解决此问题。还可以进行局部归一化（LRN）、数据增强等操作，来增加鲁棒性。 
　　当来到了全连接层之后，可以理解为一个简单的多分类神经网络（如：BP神经网络），通过softmax函数得到最终的输出。整个模型训练完毕。 
　　两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的神经网络神经元的连接方式是一样的： 

### 6. 输出层

## 三、要点分析

CNN 具有 4 个特点:局部连接、权值共享、池化操作及多层 。
CNN 能够通过多层非线性变换，从大数据中自动学习特征，从而代替手工设计的特 征，且深层的结构使它具有很强的表达能力和学习能力 。

先介绍卷积层遇到的几个名词：
![arch](arch-1.png)

- 深度/depth（解释见下图）
- 步长/stride （窗口一次滑动的长度）
- 填充值/zero-padding



激活函数（又叫激励函数，后面就全部统称为激活函数）是模型整个结构中的非线性扭曲力，神经网络的每层都会有一个激活函数。
简单来说：1，加入非线性因素    2，充分组合特征。
在神经网络中，如果不对上一层结点的输出做非线性转换的话（其实相当于激活函数为 f(x)=x），再深的网络也是线性模型，只能把输入线性组合再输出，不能学习到复杂的映射关系，而这种情况就是最原始的感知机（perceptron），那么网络的逼近能力就相当有限，因此需要使用激活函数这个非线性函数做转换，这样深层神经网络表达能力就更加强大了（不再是输入的线性组合，而是几乎可以逼近任意函数）。

![激励函数](JLHS.png)
![激励函数结构](JLHS-2.png)

![SigMoid](sigmoid.png)
![tanh](tanh.png)
![tanh-2](tanh-2.png)
![ReLU](ReLU.png)
![ELU](ELU.png)

## 参考文献

- [深度学习笔记——常用的激活（激励）函数](https://www.cnblogs.com/wj-1314/p/12015278.html)
- [深入学习卷积神经网络中卷积层和池化层的意义](https://www.cnblogs.com/wj-1314/p/9593364.html)
- [卷积的本质及物理意义](https://www.zdaiot.com/MachineLearning/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%9C%AC%E8%B4%A8%E5%8F%8A%E7%89%A9%E7%90%86%E6%84%8F%E4%B9%89%EF%BC%88%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%EF%BC%89/)

## 文档下载

- [卷积神经网络研究综述](卷积神经网络研究综述.pdf)

## CNN 示例

- [LeNet-5 示例](http://yann.lecun.com/exdb/lenet/index.html)

### 视频资料

- []