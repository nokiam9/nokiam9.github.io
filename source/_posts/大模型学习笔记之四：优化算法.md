---
title: 大模型学习笔记之四：优化算法
date: 2024-08-03 17:56:02
tags:
---

## 一、概述

机器学习的优化（目标），简单来说是：搜索模型的一组参数 w，它能显著地降低代价函数 J(w)，该代价函数通常包括整个训练集上的性能评估（经验风险）和额外的正则化（结构风险）。与传统优化不同，它不是简单地根据数据的求解最优解，在大多数机器学习问题中，我们关注的是测试集（未知数据）上性能度量P的优化。

对于模型，测试集是未知，我们只能通过优化训练集的性能度量P_train，在独立同分布基础假设下，期望测试集也有较好的性能（泛化效果），这意味并不是一味追求训练集的最优解。
另外，有些情况性能度量P（比如分类误差f1-score）并不能高效地优化，在这种情况下，我们通常会优化替代损失函数 (surrogate loss function)。例如，负对数似然通常用作 0 − 1 分类损失的替代。

## 二、主要算法

### 1. 最小二乘法

### 2. 梯度下降法

### 3. 随机梯度下降法

### 4. Momentum 动量法

### 5. Nesterov 动量法

### 6. Adagrad

### 7. RMSProp

### 8. Adam 算法

### 9. 牛顿法

### 遗传算法

## 三、简要分析

---

## 参考文献

- [一文概览神经网络优化算法](https://cloud.tencent.com/developer/article/2014684)
- [从梯度下降到 Adam！一文看懂各种神经网络优化算法](https://www.cvmart.net/community/detail/5691)
  