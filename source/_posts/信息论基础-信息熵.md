---
title: 信息论基础 - 信息熵
date: 2024-07-16 20:26:44
tags:
---

1948年，美国数学家克劳德·香农发表论文《通信的数学理论》（A Mathematical Theory of Communication），奠定了信息论的基础。

信息论回答了通信理论的两个基本问题：一是数据压缩的极限（答案：熵$H$）；二是通信传输速率的临界（答案：信道容量$C$）

- 香农第一定理：无失真信源编码定理，或变长码信源编码定理
    将原始信源符号转化为新的码符号，使码符号尽量服从等概分布，从而每个码符号所携带的信息量达到最大，进而可以用尽量少的码符号传输信源信息。即信源的信息熵 $H_r(S)$ 是无失真信源压缩的极限值。
- 香农第二定理：有噪信道编码定理
    当信道的信息传输率不超过信道容量（$R<C$）时，采用合适的信道编码方法可以实现任意高的传输可靠性，但若信息传输率超过了信道容量（$R>C$），就不可能实现可靠的传输。
- 香农第三定理：保真度准则下的有失真信源编码定理，或有损信源编码定理
    只要码长足够长，总可以找到一种信源编码，使编码后的信息传输率 $R'$ 略大于率失真函数 $R(D)$，而码的平均失真度不大于给定的允许失真度，即$D' \leq D$

> 样本空间是概率论的基础，包含了所有可能的实验结果或观察结果。
> 本文使用 $:=$ 表示“**定义**”，区别与一般意义的“相等”，或“赋值”

## 一、定义在事件上的函数

### 自信息（Self-information）

自信息是一个事件的信息量的度量，基本思想是概率越小，事件蕴含的信息量越大，定义为：
$$I(x) := -\log P_X(x)$$

举个例子，假定北京天气是晴天的概率是 80%，下雨的概率是 20%，那么：

- 天气预报说“今天是晴天”，提供的信息量就是 $-\log 0.8=0.322$，说明这是一个大概率事件
- 天气预报说“今天是雨天”，提供的信息量就是 $-\log 0.2=2.322$，说明这是一个小概率事件
  
一个 100% 概率发生的事件，其信息量就是 $-\log 1=0$；“国足踢进了世界杯“这种非常小概率的事件，一定有很多曲折的内幕，其蕴含的信息量就非常大；最极端的，如果发生概率为 0，那么一旦发生就意味着我们发现一个从不知道的新世界，信息量就是无穷大了！

满足如下性质：

- 非负性：$I(x) \geq 0 $
- 单调（递减）性：如果 $P(a) < P(b)$，则$I(a) > I(b)$
- 独立可加性：当事件 $a$ 和 $b$ 相互独立时，$I(a,b) = I(a) + I(b)$

### 联合自信息（Joint Information）

定义：给定样本空间 X 和 Y，两个事件 $x$,$y$ 的概率的联合自信息为：
$$I(x,y) := -\log P_{XY}(xy)$$

### 条件自信息（Conditional Information）

定义：给定样本空间 X 和 Y，当给定事件 $y$ 发生的条件下，事件 $x$ 的条件自信息为：
$$I(x|y) := -\log P_{X|Y}(x|y)$$

满足如下性质：

- $I(x,y) = I(x|y) + I(y) = I(y|x) + I(x) $

### 互信息（Mutual Information）

定义：已知 $I(x)$是 $x$ 事件所含有的信息量，$I(x|y)$ 是 $x$ 事件在给定 $y$ 事件发生后的信息量，那么两者的差值就是 $y$ 事件带给 $x$ 事件的信息量（增益）:
$$ I_{X;Y}(x,y) := I_X(x) - I_{X|Y}(x|y) = \log \frac {P_{X|Y}(x|y)}{P_X(x)} = \log \frac {P_{XY}(xy)}{P_X(x)P_Y(y)}$$

互信息是用来衡量随机变量之间的依赖性的度量，满足如下性质：

- 对称性：$I(x;y) = I(y;x)$
- 当事件 $x$ 和 $y$ 相互独立时，$I(x;y) = 0$；即 $y$ 无法给 $x$ 带来信息（增益）
- 可正可负
- $I(x;y) \leq \frac {I(x)}{I(y)}$

## 二、定义在概率分布上的函数

### 信息熵（Information Entropy）

定义为一个样本空间上所有随机事件（随机变量是离散的）的自信息的期望，熵在物理意义上是平均意义下对随机事件不确定性/信息量的度量，计算机意义上是平均意义上对随机变量的编码长度。
换句话说，事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望），就是这个分布产生的信息量的平均值，即为信息熵。

![XXS](xxs0.png)

如果样本空间是离散变量，也称为香农熵（Shannon Entropy），定义为：
$$H(x) := E_X[I(X)] = - \sum_{i=1}^{n} p(x_i) \log {p(x_i)}$$
> 在均匀分布时，$H(X)$取得最大值

如果样本空间是连续变量，也称为差分熵（Differential Entropy），定义为：
$$h(x) := -\int p(x) \log p(x)dx$$
> 在正态分布时，$h(x)$取得最大值

还是以天气预报为例，考虑几种不同的概率分布：

- 如果晴天概率80%，雨天概率20%，信息熵为 $-(0.8 \times log_{2}0.8 + 0.2 \times log_{2}0.2) =0.7219$
- 如果晴天和雨天的概率都是50%，信息熵为 $-(0.5 \times log_{2}0.5 + 0.5 \times log_{2}0.5) = 1$
- 再搞复杂一点，如果有四种天气，分别是晴天、阴天、小雨和大雨，其发生概率都是25%，则信息熵为 $-(0.25 \times log_{2}0.25 + 0.25 \times log_{2}0.25 + 0.25 \times log_{2}0.25 + 0.25 \times log_{2}0.25 = 2$

![XXS](xxs1.png)

信息熵的现实意义，可以有多个角度的理解：

1. 对于一个给定的（预测）概率分布，信息熵是**在结果出来之前**，对可能产生的信息量的期望，代表着基于该预测条件下，对系统不确定性的评估数值，即：信息熵越高，系统的不确定性更高，而对于一个确定的信息，信息熵为零，信息量为零。
   一般来说，如果其概率分布为一个均匀分布，则信息熵最大。下图展示了一个二元信源的熵函数：
    在上面的例子中，预测二的概率分布比预测一更平均，因此不确定性更强；预测三虽然也是平均分布，但天气的状态数量预测二更多，也说明其不确定性更强。
2. 在信息论中，信息熵代表着根据信息的概率分布对信息编码所需要的**最短**平均编码长度。
    ![alt text](xxs2.png)
    对于上面的预测三，如果 4 种天气的概率系统，信息熵为 2，意味着最少需要 2 位编码表示；
    对于上图的不均匀分布，最优编码是 0、10、110、111，此时平均编码长度为 1.75 位，与信息熵完全相等。

### 3. 联合熵（Joint Entropy）

联合熵是衡量一组变量的不确定性的指标，显然其与联合概率有关。数学定义为：
$$ H(X,Y) := -\sum_{i=1}^{n} \sum_{y=1}^{m} p(x_i,y_j) log{p(x_i,y_j)}$$

这个公式也比较容易理解，如果 $X$ 与 $Y$ 是独立的，根据概率论的知识可知，$P(x,y) = P(x)*P(y)$，则可得到 $H(X, Y) = H(X) + H(Y)$ , 可从两个变量推广到 N 个变量的情况。

> 联合熵的物理意义是：观察一个多个随机变量的随机系统获得的信息量。

### 4. 条件熵（Conditional Entropy）

在给定 $Y = \\{y_1, y_2, y_3 … y_n\\}$ 发生的前提下，求事件 $X = \\{x_1, x_2, x_3 … x_m\\}$ 的熵，称为 $X$ 的条件熵，用来衡量 $Y$ 发生的不确定性, 此时用 $H(X|Y)$ 来表示 $X$ 的条件。
与熵是一样的，条件熵越大，表示该事件的不确定性越大。数学定义是：
$$ H(X|Y) := -\sum_{i=1}^{n} \sum_{y=1}^{m} P(x_i,y_j) log \frac {1}{P(x_i|y_j)}$$

> 条件熵的物理意义就是：在得知某一确定信息的基础上获取另外一个信息时所获得的信息量。

### 5. 互信息（Mutual Information）

互信息（Mutual Information）是衡量已知一个变量时，另一个变量不确定性的减少程度。两个离散随机变量  X和 Y的互信息定义为
$$ I(X,Y) = \sum_{i=1}^{n} \sum_{j=1}^{m} p(x,y)log \frac {p(x,y)}{p(x)p(y)} $$

如果 X 和  Y 互相独立，即  X 和 Y 之间互相不提供任何信息，反之亦然，因此他们的互信息为 0。

对于两个随机变量 𝑋,𝑌，它们的互信息可以定义为 𝑋,𝑌 的联合分布和对立分布乘积的相对熵。


经过变形和计算可以得到互信息：

𝐼(𝑋;𝑌)=𝐻(𝑋)+𝐻(𝑌)−𝐻(𝑋,𝑌)
互信息的意义是衡量 𝑋 到 𝑌 的不确定性的减少程度，(衡量随机变量之间相互依赖程度的度量。)

另外互信息是对称的（symmetric），也就是𝐼(𝑋;𝑌)=𝐼(𝑌;𝑋)，所以互信息不能用于确定信息流的方向。

使用概率的加和规则和乘积规则,我们看到互信息和条件熵之间的关系为

$$I(X,Y) = H(X)−H(X|Y) = H(Y) - H(Y|X)$$

### 6. 小结

对于随机变量 𝑋,𝑌，它们的熵、联合熵、条件熵以及互信息之间的关系是：
![sum](sum0.png)

其中，左边的圆形区域表示随机变量 𝑋 的熵，右边的圆形区域表示随机变量 𝑌 的熵。
左边的 𝐻(𝑋∣𝑌) 区域表示在随机变量 𝑌 给定的条件下随机变量 𝑋 的条件熵；左边的 𝐻(𝑌∣𝑋) 区域表示在随机变量 𝑋 给定的条件下随机变量 𝑌 的条件熵。两个圆中间相交的部分表示随机变量 𝑋,𝑌 的互信息。两个圆构成的整体部分表示 𝑋,𝑌 的联合熵。

![sum](sum1.png)

## 二、数学原理的进阶篇

### 1. 相对熵（Relative Entropy）

相对熵是衡量两个不同分布之间的**距离**或者相似的一个指标。也称为 KL散度（Kullback-Leibler Divergence）。
假设 $P(x)$ 是目标分布，$Q(x)$ 是评价分布，衡量$Q(x)$与目标分布 $P(x)$ 之间的距离，就可以使用这个指标实现。公式是，
$$D_{KL}(p||q) = \sum_{i=1}^n p(x_i)log(\frac {p(x_i)}{q(x_i)})$$

相对熵是衡量两个不同分布之间的“距离”或者相似的一个指标。假设这两个分布是P(x)和Q(x)，其中P(x)是目标分布，Q(x)是评价分布，衡量Q(x)与目标分布P(x)之间的距离，就可以使用这个指标实现。公式是，
`相对熵 = 交叉熵 - 信息熵`

![KL](KL.png)

#### KL 散度的特性

1. 非对称性：$ D_{KL}(P \\| Q) \neq D_{KL}(Q \Vert P) $。即 KL 散度不是一个对称量
2. 非负性：$ D_{KL}(P || Q) \geqslant 0 $，当且仅当 $P(x) = Q(x) $ 时等号成立。根据[吉布斯不等式](https://allenwind.github.io/blog/6631/)可以证明

KL 散度 = H(P,Q) - H(P) = 交叉熵 - 信息熵

在模型的训练阶段，输入数据和标签常常已经确定，那么真实概率分布P(X)是确定，所以信息熵在这里是一个常量。
由于KL散度的值表示真实概率分布P(x)与预测概率分布Q(x)之间的差异，值越小表示两个分布之间的差异性越小，所以目标变成最小化KL散度。但是由于交叉熵等于KL散度加上一个常量，且公式相比KL散度更加容易，因此最小化KL散度与最小化交叉熵的效果是一致的，且优化交叉熵的计算量小于优化KL散度。


联合熵是衡量一组变量的不确定性的指标，显然其与联合概率有关。公式为，

### 2. 交叉熵(Cross Entropy)

交叉熵是在机器学习的分类任务里被广泛使用的目标函数(objective function)/损失函数(loss function)。上面提到了，理论上相对熵才是衡量两个分布是否相近的指标，而在实际中为什么使用的是交叉熵呢？通过下面的推导公式，可以得出结论

给定一个策略，交叉熵就是在该策略下猜中颜色所需要的问题的期望值。交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出成本的大小。交叉的字面意思在于：真实分布与非真实分布的交叉。给定一个方案，越优的策略，交叉熵越低，具有最低的交叉熵策略就是最优的策略。而在此时，交叉熵=信息熵。因此，我们通常需要最小化交叉熵，也间接证明了我们的算法所算出的非真实分布接近真实分布。交叉熵也叫对数似然


### 3. 小结


交叉熵：预测越准确，交叉熵越小；交叉熵只与真实标签的预测概率相关
$$ H(P,q) = - \sum_{i=1}^{n} p(x_i)logq(x_i) $$
二分类公式：$ H(P,Q) = -(plogq + (1-p)log(1-q))$

对比熵和交叉熵的公式，我们可以发现两者在形式上非常相似，都涉及到了概率乘以对数概率的形式。主要区别在于：

熵H ( X ) H(X)H(X)只依赖于一个概率分布，它衡量的是这个概率分布本身的不确定性。
对应的，交叉熵H ( P , Q ) H(P, Q)H(P,Q)依赖于两个概率分布，它衡量的是一个概率分布相对于另一个概率分布的不确定性。

简单的说，交叉熵背后是KL距离的计算，KL距离可以用来衡量两个概率分布的"距离"。但是KL距离并不是度量，不满足交换律，即KL(p,q)不等于KL(q,p)，含义也是不一样的。

区别
概念上：交叉熵衡量的是在一个分布下，编码另一个分布所需的信息量；而 KL 散度衡量的是两个概率分布之间的信息损失或差异。
计算公式：交叉熵的计算公式中不直接涉及真实分布P PP相对于预测分布Q QQ的比率，而KL散度的公式直接涉及到这个比率P ( x ) Q ( x ) \frac{P(x)}{Q(x)} Q(x)P(x)。
对称性：交叉熵是对称的，没有明确的方向性；KL散度是非对称的，明确区分了哪个是“真实”分布，哪个是“预测”或“近似”分布。
关系：二者之间有密切关系，实际上，KL散度可以被视为两个分布之间交叉熵和真实分布熵的差值：
D K L ( P ∥ Q ) = H ( P , Q ) − H ( P ) D_{KL}(P \Vert Q) = H(P, Q) - H(P) D KL

---

## 参考文献

- [通俗理解熵与交叉熵](https://zodiac911.github.io/blog/entropy-and-crossentropy.html#%E4%BA%A4%E5%8F%89%E7%86%B5-Cross-Entropy)
- [Entropy](https://www.cnblogs.com/Blackteaxx/p/18164220)
- [熵 (Entropy)](https://medium.com/@eric_zhu/%E7%86%B5-entropy-8299ccce0cf2)
- [信息熵、交叉熵、KL-散度、联合熵、条件熵和互信息](https://gulico.github.io/2020/07/20/xinxilun/)
- [信息论基础（熵，互信息，交叉熵）- 老羊肖恩](https://www.jianshu.com/p/71bd778dfb5a)
- [机器学习里的信息论](https://www.cnblogs.com/ZihanZhang/p/16049215.html)
- [损失函数的解释](https://paulxiong.medium.com/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E8%A7%A3%E9%87%8A-c2b6f165c842)
