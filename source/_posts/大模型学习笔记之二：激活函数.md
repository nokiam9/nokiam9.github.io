---
title: 大模型学习笔记之二：激活函数
date: 2024-06-30 13:27:50
tags:
---

## Softmax 函数

Softmax 是一种激活函数，它可以将一个数值向量**归一化**为一个概率分布向量，且各个概率之和为 1。

$$
Softmax(z_i)=\frac {e^{z_i}} {\sum\limits_{j=1}^{n} e^{z_j}}
$$

正常的 max 函数仅输出最大值，但 Softmax 函数确保较小的值具有较小的概率，并且不会直接丢弃。我们可以认为它是argmax 函数的概率版本或“soft”版本。
Softmax 函数的分母结合了原始输出值的所有因子，这意味着 Softmax 函数获得的各种概率彼此相关。

![softmax](softmax.png)

Softmax 通常作为神经网络的**最后**一层，用于多分类问题的输出，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类，常常和交叉熵损失函数一起结合使用。

下面是使用Python进行函数计算的示例代码：

```python
import math
z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]
z_exp = [math.exp(i) for i in z]  
print(z_exp)  # Result: [2.72, 7.39, 20.09, 54.6, 2.72, 7.39, 20.09] 
sum_z_exp = sum(z_exp)  
print(sum_z_exp)  # Result: 114.98 
softmax = [round(i / sum_z_exp, 3) for i in z_exp]
print(softmax)  # Result: [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]
```
