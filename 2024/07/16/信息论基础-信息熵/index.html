<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"nokiam9.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.27.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="熵（Entropy）最早是物理学的概念，用于表示一个热力学系统的无序程度。1948年，美国数学家克劳德·香农发表论文《通信的数学理论》（A Mathematical Theory of Communication），将统计物理中熵的概念引申到信道通信的过程中，从而奠定了信息论的基础。  信息论回答了通信理论的两个基本问题：一是数据压缩的极限（答案：熵$H$）；二是通信传输速率的临界（答案：信道容量">
<meta property="og:type" content="article">
<meta property="og:title" content="信息论基础 - 信息熵">
<meta property="og:url" content="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/index.html">
<meta property="og:site_name" content="Alex的技术博客">
<meta property="og:description" content="熵（Entropy）最早是物理学的概念，用于表示一个热力学系统的无序程度。1948年，美国数学家克劳德·香农发表论文《通信的数学理论》（A Mathematical Theory of Communication），将统计物理中熵的概念引申到信道通信的过程中，从而奠定了信息论的基础。  信息论回答了通信理论的两个基本问题：一是数据压缩的极限（答案：熵$H$）；二是通信传输速率的临界（答案：信道容量">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/xxs0.png">
<meta property="og:image" content="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/xxs2.png">
<meta property="og:image" content="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/sum1.png">
<meta property="og:image" content="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/KL.png">
<meta property="og:image" content="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/KL2.png">
<meta property="og:image" content="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/demo.png">
<meta property="article:published_time" content="2024-07-16T12:26:44.000Z">
<meta property="article:modified_time" content="2026-02-23T08:34:16.017Z">
<meta property="article:author" content="Alex Sun">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/xxs0.png">


<link rel="canonical" href="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/","path":"2024/07/16/信息论基础-信息熵/","title":"信息论基础 - 信息熵"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>信息论基础 - 信息熵 | Alex的技术博客</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script><script src="/js/bookmark.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.1/pdfobject.min.js","integrity":"sha256-jI72I8ZLVflVOisZIOaLvRew3tyvzeu6aZXFm7P7dEo="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js" defer></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.10.1/mermaid.min.js","integrity":"sha256-BmQmdWDS8X2OTbrwELWK366LV6escyWhHHe0XCTU/Hk="}}</script>
  <script src="/js/third-party/tags/mermaid.js" defer></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Alex的技术博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-text">一、信息熵的定义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%87%AA%E4%BF%A1%E6%81%AF%EF%BC%88Self-information%EF%BC%89"><span class="nav-text">1. 自信息（Self-information）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BF%A1%E6%81%AF%E7%86%B5%EF%BC%88Information-Entropy%EF%BC%89"><span class="nav-text">2. 信息熵（Information Entropy）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%B7%AE%E5%88%86%E7%86%B5%EF%BC%88Differential-Entropy%EF%BC%89"><span class="nav-text">3. 差分熵（Differential Entropy）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E8%81%94%E5%90%88%E7%86%B5%E3%80%81%E6%9D%A1%E4%BB%B6%E7%86%B5%E5%92%8C%E4%BA%92%E4%BF%A1%E6%81%AF"><span class="nav-text">二、联合熵、条件熵和互信息</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%81%94%E5%90%88%E7%86%B5%EF%BC%88Joint-Entropy%EF%BC%89"><span class="nav-text">1. 联合熵（Joint Entropy）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%9D%A1%E4%BB%B6%E7%86%B5%EF%BC%88Conditional-Entropy%EF%BC%89"><span class="nav-text">2. 条件熵（Conditional Entropy）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%BA%92%E4%BF%A1%E6%81%AF%EF%BC%88Mutual-Information%EF%BC%89"><span class="nav-text">3. 互信息（Mutual Information）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%86%B5%E5%92%8C%E4%BA%92%E4%BF%A1%E6%81%AF%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-text">4. 熵和互信息的关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E7%9B%B8%E5%AF%B9%E7%86%B5-%E4%B8%8D%E5%90%8C%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E7%9A%84%E6%AF%94%E8%BE%83"><span class="nav-text">三、相对熵 - 不同概率分布的比较</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-KL%E6%95%A3%E5%BA%A6%EF%BC%88Kullback-Leibler-Divergence%EF%BC%8C%E6%88%96-%E7%9B%B8%E5%AF%B9%E7%86%B5-Relative-Entropy%EF%BC%89"><span class="nav-text">1. KL散度（Kullback-Leibler Divergence，或 相对熵 - Relative Entropy）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BA%A4%E5%8F%89%E7%86%B5%EF%BC%88Cross-Entropy%EF%BC%89"><span class="nav-text">2. 交叉熵（Cross Entropy）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-JS%E6%95%A3%E5%BA%A6%EF%BC%88Jensen-Shannon-Divergence%EF%BC%89"><span class="nav-text">3. JS散度（Jensen-Shannon Divergence）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E5%BA%94%E7%94%A8%E5%88%86%E6%9E%90"><span class="nav-text">四、应用分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95%E4%B8%80%EF%BC%9A%E5%87%A0%E4%B8%AA%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A"><span class="nav-text">附录一：几个名词解释</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Divergence-%E6%95%A3%E5%BA%A6"><span class="nav-text">Divergence - 散度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Metric-%E5%BA%A6%E9%87%8F"><span class="nav-text">Metric - 度量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95%E4%BA%8C%EF%BC%9A%E5%90%89%E5%B8%83%E6%96%AF%E4%B8%8D%E7%AD%89%E5%BC%8F%EF%BC%88Gibbs%E2%80%99-Inequality%EF%BC%89"><span class="nav-text">附录二：吉布斯不等式（Gibbs’ Inequality）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95%E4%B8%89%EF%BC%9A%E6%9C%80%E5%A4%A7%E7%86%B5%E6%80%9D%E6%83%B3-Maximum-Entropy-Principle"><span class="nav-text">附录三：最大熵思想 (Maximum Entropy Principle)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95%E5%9B%9B%EF%BC%9A%E7%83%AD%E5%8A%9B%E5%AD%A6%E7%AC%AC%E4%BA%8C%E5%AE%9A%E5%BE%8B%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="nav-text">附录四：热力学第二定律的解释</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-text">参考文献</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Alex Sun</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">115</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://nokiam9.github.io/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Alex Sun">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex的技术博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="信息论基础 - 信息熵 | Alex的技术博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          信息论基础 - 信息熵
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-07-16 20:26:44" itemprop="dateCreated datePublished" datetime="2024-07-16T20:26:44+08:00">2024-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2026-02-23 16:34:16" itemprop="dateModified" datetime="2026-02-23T16:34:16+08:00">2026-02-23</time>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>19 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>熵（Entropy）最早是物理学的概念，用于表示一个热力学系统的无序程度。1948年，美国数学家克劳德·香农发表论文《通信的数学理论》（A Mathematical Theory of Communication），将统计物理中熵的概念引申到信道通信的过程中，从而奠定了信息论的基础。</p>
<p>信息论回答了通信理论的两个基本问题：一是数据压缩的极限（答案：熵$H$）；二是通信传输速率的临界（答案：信道容量$C$）</p>
<ul>
<li>香农第一定理：无失真信源编码定理，或变长码信源编码定理<br>  将原始信源符号转化为新的码符号，使码符号尽量服从等概分布，从而每个码符号所携带的信息量达到最大，进而可以用尽量少的码符号传输信源信息。即信源的信息熵 $H_r(S)$ 是无失真信源压缩的极限值。</li>
<li>香农第二定理：有噪信道编码定理<br>  当信道的信息传输率不超过信道容量（$R&lt;C$）时，采用合适的信道编码方法可以实现任意高的传输可靠性，但若信息传输率超过了信道容量（$R&gt;C$），就不可能实现可靠的传输。</li>
<li>香农第三定理：保真度准则下的有失真信源编码定理，或有损信源编码定理<br>  只要码长足够长，总可以找到一种信源编码，使编码后的信息传输率 $R’$ 略大于率失真函数 $R(D)$，而码的平均失真度不大于给定的允许失真度，即$D’ \leq D$</li>
</ul>
<blockquote>
<p>样本空间是概率论的基础，包含了所有可能的实验结果或观察结果。<br>本文使用 $:&#x3D;$ 表示“<strong>定义</strong>”，区别与一般意义的“相等”，或“赋值”</p>
</blockquote>
<h2 id="一、信息熵的定义"><a href="#一、信息熵的定义" class="headerlink" title="一、信息熵的定义"></a>一、信息熵的定义</h2><h3 id="1-自信息（Self-information）"><a href="#1-自信息（Self-information）" class="headerlink" title="1. 自信息（Self-information）"></a>1. 自信息（Self-information）</h3><p>自信息是一个事件的信息量的度量，基本思想是概率越小，事件蕴含的信息量越大，定义为：<br>$$I(x) :&#x3D; -\log P_X(x)$$</p>
<p>举个例子，假定北京天气是晴天的概率是 80%，下雨的概率是 20%，那么：</p>
<ul>
<li>天气预报说“今天是晴天”，提供的信息量就是 $-\log 0.8&#x3D;0.322$，说明这是一个大概率事件</li>
<li>天气预报说“今天是雨天”，提供的信息量就是 $-\log 0.2&#x3D;2.322$，说明这是一个小概率事件</li>
</ul>
<p>一个 100% 概率发生的事件，其信息量就是 $-\log 1&#x3D;0$；“国足踢进了世界杯“这种非常小概率的事件，一定有很多曲折的内幕，其蕴含的信息量就非常大；最极端的，如果发生概率为 0，那么一旦发生就意味着我们发现一个从不知道的新世界，信息量就是无穷大了！</p>
<p>满足如下性质：</p>
<ul>
<li>非负性：$I(x) \geq 0 $</li>
<li>单调（递减）性：如果 $P(a) &lt; P(b)$，则$I(a) &gt; I(b)$</li>
<li>独立可加性：当事件 $a$ 和 $b$ 相互独立时，$I(a,b) &#x3D; I(a) + I(b)$</li>
</ul>
<h3 id="2-信息熵（Information-Entropy）"><a href="#2-信息熵（Information-Entropy）" class="headerlink" title="2. 信息熵（Information Entropy）"></a>2. 信息熵（Information Entropy）</h3><p>定义为一个样本空间上所有随机事件（随机变量是离散的）的自信息的期望，熵在物理意义上是平均意义下对随机事件不确定性&#x2F;信息量的度量，计算机意义上是平均意义上对随机变量的编码长度。<br>换句话说，事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望），就是这个分布产生的信息量的平均值，即为信息熵。</p>
<p><img src="/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/xxs0.png" alt="XXS"></p>
<p>如果样本空间是离散变量，也称为香农熵（Shannon Entropy），定义为：<br>$$H(x) :&#x3D; E_X[I(X)] &#x3D; - \sum_{x \in X} p(x) \log {p(x)}$$</p>
<p>香农熵满足以下性质：</p>
<ul>
<li>非负性：$H(X) \geq 0$</li>
<li>概率密度函数为平均分布时，$H(X)$ 取得最大值</li>
</ul>
<blockquote>
<p>信息论的物理意义：根据信息的概率分布对信息编码所需要的<strong>最短平均编码长度</strong>。</p>
</blockquote>
<p>还是以天气预报为例，考虑几种不同的概率分布：</p>
<ul>
<li>如果晴天概率80%，雨天概率20%，信息熵为 $-(0.8 \times log_{2}0.8 + 0.2 \times log_{2}0.2) &#x3D;0.7219$</li>
<li>如果晴天和雨天的概率都是50%，信息熵为 $-(0.5 \times log_{2}0.5 + 0.5 \times log_{2}0.5) &#x3D; 1$</li>
<li>再搞复杂一点，如果有四种天气，分别是晴天、阴天、小雨和大雨，其发生概率都是25%，则信息熵为 $-(0.25 \times log_{2}0.25 + 0.25 \times log_{2}0.25 + 0.25 \times log_{2}0.25 + 0.25 \times log_{2}0.25 &#x3D; 2$</li>
</ul>
<p>对于一个给定的（预测）概率分布，信息熵是对可能产生的信息量的期望，代表着对系统不确定性的评估数值。在上面的例子中，预测二的概率分布比预测一更平均，因此不确定性更强；与预测二相比，预测三同样是平均分布，但天气的状态数量更多，也说明其不确定性更高。</p>
<p><img src="/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/xxs2.png" alt="alt text"><br>预测三的 4 种天气为均匀分布，其信息熵为 2，意味着最少需要 2 位编码表示；<br>而上图的不均匀分布，最优编码是 0、10、110、111，平均编码长度为 1.75，与信息熵相等。<br>$ 1 \times 0.5+2 \times 0.25+3 \times 0.125+3 \times 0.125&#x3D;1.75 $</p>
<h3 id="3-差分熵（Differential-Entropy）"><a href="#3-差分熵（Differential-Entropy）" class="headerlink" title="3. 差分熵（Differential Entropy）"></a>3. 差分熵（Differential Entropy）</h3><p>香农熵是基于离散变量的，如果拓展到连续变量，称为差分熵（Differential Entropy），定义为：<br>$$h(x) :&#x3D; -\int p(x) \log p(x)dx$$</p>
<p>微分熵满足以下性质：</p>
<ul>
<li>概率密度函数为正态（高斯）分布时，$h(x)$取得最大值。</li>
<li><strong>不满足非负性</strong>。<br>  差分熵可以是正的，也可以是负的，甚至是未定义的。例如，柯西分布（Cauchy distribution）就是一个具有未定义的差分熵的分布，因为它有一个尖峰在中心，并且尾部衰减得非常慢，导致其概率密度函数在中心附近为零，但在整个实数线上积分不为零。请参见 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Differential_entropy">Differential entropy - Wiki</a>。</li>
</ul>
<h2 id="二、联合熵、条件熵和互信息"><a href="#二、联合熵、条件熵和互信息" class="headerlink" title="二、联合熵、条件熵和互信息"></a>二、联合熵、条件熵和互信息</h2><p>上面定义了一个随机变量的熵，现在推广到两个或多个随机变量。</p>
<h3 id="1-联合熵（Joint-Entropy）"><a href="#1-联合熵（Joint-Entropy）" class="headerlink" title="1. 联合熵（Joint Entropy）"></a>1. 联合熵（Joint Entropy）</h3><p>联合熵是衡量一组变量的不确定性的指标，显然其与联合概率有关。数学定义为：<br>$$ H(X,Y) :&#x3D; E[I(X,Y)] &#x3D; -\sum_{x \in X} \sum_{y \in Y} p(x,y) \log {p(x,y)}$$</p>
<p>满足以下性质：</p>
<ul>
<li>如果 $X$ 与 $Y$ 是独立的，$H(X, Y) &#x3D; H(X) + H(Y)$ , 并推广到 N 个变量</li>
</ul>
<blockquote>
<p>信息论的物理意义：观察一个包含多个随机变量的随机系统获得的信息量。</p>
</blockquote>
<h3 id="2-条件熵（Conditional-Entropy）"><a href="#2-条件熵（Conditional-Entropy）" class="headerlink" title="2. 条件熵（Conditional Entropy）"></a>2. 条件熵（Conditional Entropy）</h3><p>在给定 $Y &#x3D; \{y_1, y_2, y_3 … y_n\}$ 发生的前提下，求事件 $X &#x3D; \{x_1, x_2, x_3 … x_m\}$ 的熵，称为 $X$ 的条件熵，用来衡量 $Y$ 发生的不确定性, 此时用 $H(X|Y)$ 来表示 $X$ 的条件。数学定义是：<br>$$ H(X|Y) :&#x3D; -\sum_{x \in X} \sum_{y \in Y} p(x,y) \log p(x|y) &#x3D;  -\sum_{x \in X} \sum_{y \in Y} p(x,y) \log \frac {p(x,y)}{p(x)} $$</p>
<p>满足以下性质：</p>
<ul>
<li>链式规则：$H(Y|X) &#x3D; H(X,Y) - H(X)$</li>
<li>贝叶斯规则：$H(Y|X) &#x3D; H(X|Y) + H(Y) - H(X)$</li>
</ul>
<blockquote>
<p>信息论的物理意义：在得知某一确定信息的基础上，获取另外一个信息时所获得的信息量。</p>
</blockquote>
<h3 id="3-互信息（Mutual-Information）"><a href="#3-互信息（Mutual-Information）" class="headerlink" title="3. 互信息（Mutual Information）"></a>3. 互信息（Mutual Information）</h3><p>定义：已知 $I(x)$是 $x$ 事件所含有的信息量，$I(x|y)$ 是 $x$ 事件在给定 $y$ 事件发生后的信息量，那么两者的差值就是 $y$ 事件带给 $x$ 事件的信息量（增益）:<br>$$ I(X;Y) :&#x3D; I(X) - I(X|Y) &#x3D;  -\sum_{x \in X} \sum_{y \in Y} p(x,y) \log \frac {p(x,y)}{p(x)p(y)} $$</p>
<p>互信息是用来衡量随机变量之间的依赖性的度量，满足如下性质：</p>
<ul>
<li>对称性：$I(x;y) &#x3D; I(y;x)$</li>
<li>当事件 $x$ 和 $y$ 相互独立时，$I(x;y) &#x3D; 0$；即 $y$ 无法给 $x$ 带来信息（增益）</li>
<li>可正可负</li>
<li>$I(x;y) \leq \frac {I(x)}{I(y)}$</li>
</ul>
<blockquote>
<p>信息论的物理意义：互信息（信息增益） &#x3D; 信息熵 — 条件熵</p>
</blockquote>
<h3 id="4-熵和互信息的关系"><a href="#4-熵和互信息的关系" class="headerlink" title="4. 熵和互信息的关系"></a>4. 熵和互信息的关系</h3><p>对于随机变量 $X$, $Y$，它们的熵、联合熵、条件熵以及互信息之间的关系是：<br><img src="/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/sum1.png" alt="sum"></p>
<ol>
<li>$I(X;Y) &#x3D; H(X) - H(X|Y) &#x3D; H(Y) - H(Y|X)$</li>
<li>$I(X;Y) &#x3D; H(X) + H(Y) - H(X,Y)$</li>
<li>$I(X;Y) &#x3D; I(Y;X)$，满足对称性，因此不能用于确定信息流的方向</li>
<li>$I(X;X) &#x3D; H(X)$，单一变量不能给自身带来新的信息</li>
</ol>
<p>互信息就是信息增益，用于衡量随机变量之间相互依赖程度的度量。例如，构建决策树时选择某个特征变量，导致分类模型（另一个变量）的不确定性减少，也就是带来了新的信息，减少的越多说明这个特征越重要。<br>反之，如果 $X$ 和 $Y$ 互相独立，即两者之间互相不提供任何信息，则他们的互信息为 0。</p>
<h2 id="三、相对熵-不同概率分布的比较"><a href="#三、相对熵-不同概率分布的比较" class="headerlink" title="三、相对熵 - 不同概率分布的比较"></a>三、相对熵 - 不同概率分布的比较</h2><p>衡量两个概率分布之间的差异（不确定性），主要有 KL 散度、JS 散度、交叉熵和 Wasserstein 距离等指标。</p>
<h3 id="1-KL散度（Kullback-Leibler-Divergence，或-相对熵-Relative-Entropy）"><a href="#1-KL散度（Kullback-Leibler-Divergence，或-相对熵-Relative-Entropy）" class="headerlink" title="1. KL散度（Kullback-Leibler Divergence，或 相对熵 - Relative Entropy）"></a>1. KL散度（Kullback-Leibler Divergence，或 相对熵 - Relative Entropy）</h3><p>若 $P$,$Q$ 定义在同一个概率空间的不同测度，那么 KL Divergence 定义为：<br>$$D_{KL}(P||Q) :&#x3D; E_P[\log \frac {p(x)}{q(x)}] &#x3D; \sum_{i&#x3D;1}^n p(x_i)log(\frac {p(x_i)}{q(x_i)})$$<br>其中，为了保证连续性，定义 $0 \log \frac{0}{0} &#x3D; 0$，$0 \log \frac{0}{q} &#x3D; 0$。</p>
<p>以下图为例，黄色柱体是目标分布 $P(x)$，蓝色柱体是评价分布 $Q(x)$，棕色是两者的重叠部分，而本色区域就两者之间的“距离”，再基于 $P(x)$ 的概率分布“累计”得到期望值，这就是 KL 散度的直观解释。</p>
<p><img src="/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/KL.png" alt="KL"></p>
<p>KL散度又称为<strong>相对熵</strong>，信息散度，信息增益。满足以下性质：</p>
<ol>
<li>非对称性：$ D_{KL}(P \| Q) \neq D_{KL}(Q \Vert P) $。KL 散度不是严格意义的 <strong>metric</strong> 指标。</li>
<li>非负性：$ D_{KL}(P || Q) \geqslant 0 $，当且仅当 $P(x) &#x3D; Q(x) $ 时等号成立。根据<a target="_blank" rel="noopener" href="https://allenwind.github.io/blog/6631/">吉布斯不等式</a>可以证明</li>
<li>$D_{KL}(P||Q) &#x3D; H(P,Q) - H(P)$，即：<strong>相对熵 &#x3D; 交叉熵 - P的信息熵</strong></li>
</ol>
<blockquote>
<p>信息论的物理意义：使用错误分布 $Q$ 来表示真实分布 $P$ 中的样本，所使用编码的<strong>平均长度的增量</strong></p>
</blockquote>
<p>互信息是用于衡量两个随机变量之间的依赖性的度量，其实也是一种相对熵，只是它衡量的并非随机变量 $p$ 和 $q$，而是他们的联合分布 $p(x,y)$ 和边缘分布乘积 $p(x)p(y)$ 的相似程度。<br>$$ I_{X;Y}(x,y) :&#x3D; I_X(x) - I_{X|Y}(x|y) &#x3D; \log \frac {P_{X|Y}(x|y)}{P_X(x)} &#x3D; \log \frac {P_{XY}(xy)}{P_X(x)P_Y(y)}$$</p>
<h3 id="2-交叉熵（Cross-Entropy）"><a href="#2-交叉熵（Cross-Entropy）" class="headerlink" title="2. 交叉熵（Cross Entropy）"></a>2. 交叉熵（Cross Entropy）</h3><p>给定两个概率分布 $P$ 和 $Q$，$P$ 相对于 $Q$ 的交叉熵定义为：<br>$$H(P,Q) :&#x3D; E_P[-\log Q] &#x3D; -\sum_{x \in X} p(x) \log q(x)$$</p>
<p>还是以 KL散度的图为例，蓝色柱体是评价分布 $Q(x)$，不考虑黄色柱体，直接以目标分布 $P(x)$ 的概率分布“累计”得到期望值，这就是交叉熵的直观解释。这也体现了两者之间的差别就是目标分布 $P(x)$ 的信息熵。</p>
<blockquote>
<p>信息论的物理意义：使用错误分布 $Q$ 来表示真实分布 $P$ 中的样本，所使用编码的<strong>平均长度</strong></p>
</blockquote>
<h3 id="3-JS散度（Jensen-Shannon-Divergence）"><a href="#3-JS散度（Jensen-Shannon-Divergence）" class="headerlink" title="3. JS散度（Jensen-Shannon Divergence）"></a>3. JS散度（Jensen-Shannon Divergence）</h3><p>对于两个概率分布 $P$ 和 $Q$，JS 散度定义为：<br>$$D_{JS}(P||Q) &#x3D; \frac{1}{2} D_{KL}(P || M) + \frac{1}{2} D_{KL}(Q || M) $$<br>其中： $M$ 是 $P$ 和 $Q$ 的几何平均，定义为：$ M(x) &#x3D; \sqrt{P(x) \cdot Q(x)} $</p>
<p>JS散度是 KL散度的一种<strong>对称化形式</strong>，满足如下性质：</p>
<ul>
<li>非负性：$D_{JS} \geq 0$，且仅当 $P &#x3D; Q$ 时为 0</li>
<li>对称性：JS 散度是对称的，即 $D_{JS}(P || Q) &#x3D; D_{JS}(Q || P)$</li>
<li>三角不等式：对于任意三个概率分布 $P$，$Q$ 和 $R$，有：$ D_{JS}(P || R) \leq D_{JS}(P || Q) + D_{JS}(Q || R) $</li>
</ul>
<h2 id="四、应用分析"><a href="#四、应用分析" class="headerlink" title="四、应用分析"></a>四、应用分析</h2><p>KL散度可以用来衡量两个概率分布的”距离”。但是KL散度并不是标准意义的度量指标（不满足对称性），即$D_{KL}(p,q) \neq D_{KL}(q,p)$，因此应用KL散度时，必须明确区分哪个是“真实”分布，哪个是“近似（预测）”分布。</p>
<p>通常我们使用的是$D_{KL}(p,q)$（也记作$D_q(p)$），称为正向KL散度（Forward KL），要使其最小的数学表达为：<br>$$q^* &#x3D; \arg \min_q D_{KL}(p||q) &#x3D; \arg \min_q \sum_{x \in X} p(x)\log \frac {p(x)}{q(x)} $$<br>其优化重点是在$p(x)$较小的位置，$q(x)$应避免为零（将导致对数无穷大），因此其也被称为“<strong>zero avoiding</strong>”，结果是得到一个较宽的分布，峰值未必突出，但底部比较平坦。</p>
<p>反之，$D_{KL}(q,p)$（也记作$D_p(q)$）称为反向KL散度（Reverse KL），要使其最小的数学表达为：<br>$$q^* &#x3D; \arg \min_q D_{KL}(q||p) &#x3D; \arg \min_q \sum_{x \in X} q(x)\log \frac {q(x)}{p(x)} $$<br>其优化重点是在$p(x)$为零的位置，$q(x)$应尽量为零（因为分母为零导致对数无穷大，但 $0 \log \frac{0}{0}&#x3D;0$），因此其也被称为“<strong>zero forceing</strong>”，结果是得到一个较窄的分布，底部尽量低，这也往往导致峰值更突出。</p>
<p><img src="/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/KL2.png" alt="alt text"></p>
<p>综上所述，正向KL散度的分布偏向一般化，而反向KL散度的分布偏向极端化。从模型泛化的角度看，在损失函数结果相等的情况下，显然正向KL散度更有优势！</p>
<p>此外，在模型的训练阶段通常已经确定了输入数据和标签数据，即真实概率分布<code>P(X)</code>的信息熵是一个常量，此时最小化KL散度与最小化交叉熵效果是一致的，但是优化交叉熵的计算量小于优化KL散度，因此在损失函数的计算中得到广泛应用。</p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>概率分布 $P$ 和 $Q$ 分别定义在同样的样本空间 {0, 1} 上，真实分布 $P$：$P(0) &#x3D; 0.8, P(1) &#x3D; 0.2 $</p>
<ol>
<li>对于预测分布$Q1$：$Q(0) &#x3D; 0.6, Q(1) &#x3D; 0.4 $，计算交叉熵：<br>$H(P,Q) &#x3D;-(P(0) \log Q(0) + P(1) \log Q(1)) &#x3D;-(0.8 \log0.6 + 0.2 \log0.4) &#x3D; 0.8540$</li>
<li>换一个预测分布$Q2$：$Q(0) &#x3D; 0.7, Q(1) &#x3D; 0.3 $，此时交叉熵为：<br>$H(P,Q) &#x3D;-(P(0) \log Q(0) + P(1) \log Q(1)) &#x3D; -(0.8 \log0.7 + 0.1 \log0.3) &#x3D; 0.7591$</li>
<li>如果预测分布等于真实分布，$Q(0) &#x3D; 0.6, Q(1) &#x3D; 0.4 $，此时交叉熵：<br>$H(P,Q) &#x3D;-(P(0) \log Q(0) + P(1) \log Q(1)) &#x3D; -(0.8 \log0.8 + 0.2 \log0.2) &#x3D; 0.7219$</li>
<li>进一步，如果预测分布$Q4$：$Q(0) &#x3D; 0.9, Q(1) &#x3D; 0.1 $，此时交叉熵为：<br>$H(P,Q) &#x3D;-(P(0) \log Q(0) + P(1) \log Q(1)) &#x3D; -(0.8 \log0.9 + 0.2 \log0.1) &#x3D; 0.7860$</li>
</ol>
<p>分析交叉熵的数值：</p>
<ul>
<li>$H(P,Q2) &lt; H(P,Q1)$：表明相比$Q1$，$Q2$更接近真实分布；</li>
<li>$H(P,Q3)$ 是最小值：因为真实分布与其自身的交叉熵就是信息熵，$H(P,P) &#x3D; H(P)$；</li>
<li>$H(P,Q4) \approx H(P,Q2)$：表明两者与真实分布的“距离”差不多。而 $H(P,Q4) &gt; H(P,Q3)$是肯定的。</li>
</ul>
<hr>
<h2 id="附录一：几个名词解释"><a href="#附录一：几个名词解释" class="headerlink" title="附录一：几个名词解释"></a>附录一：几个名词解释</h2><h3 id="Divergence-散度"><a href="#Divergence-散度" class="headerlink" title="Divergence - 散度"></a>Divergence - 散度</h3><p>Divergence 这个词来源于拉丁语词根 diverge，意为”分叉”或”偏离”。<br>在数学中，通常指的是向量场中某一点的发散性，即向量从这一点向外散开的程度。<br>在物理学中，可以指能量或物质的发散，例如在热力学中，热量的发散。<br>反义词是 convergence ，意为”汇聚”或”趋同”，指的是从不同起点汇聚到一点或趋于一致。</p>
<h3 id="Metric-度量"><a href="#Metric-度量" class="headerlink" title="Metric - 度量"></a>Metric - 度量</h3><p>在数学中，度量空间（metric space）是具有<strong>距离（distance）</strong>这一个概念的集合，具体来说，是装配了一个称为度量的函数，用以表示此集合中任两个成员间的距离。历史上是由法国数学家莫里斯·弗雷歇在1906年于其意大利语著作《Sur quelques points du calcul fonctionnel》首次使用。其定义为：</p>
<p>$M$ 为集合，若其装配了函数 $d: M \times M \rightarrow R$，对任意 $x,y,z \in M$ 满足：</p>
<ul>
<li>同一性：$d(x,y) &#x3D; 0 \Longleftrightarrow x &#x3D; y$</li>
<li>对称性：$d(x,y) &#x3D; d(y,x)$</li>
<li>三角不等式：$d(x,z) \leq d(x,y) + d(y,z)$</li>
</ul>
<p>则称 $d$ 为定义在 $M$ 上的度量（metric），或是距离函数，且称 $(M,d)$ 为度量空间。</p>
<h2 id="附录二：吉布斯不等式（Gibbs’-Inequality）"><a href="#附录二：吉布斯不等式（Gibbs’-Inequality）" class="headerlink" title="附录二：吉布斯不等式（Gibbs’ Inequality）"></a>附录二：吉布斯不等式（Gibbs’ Inequality）</h2><p>吉布斯不等式（Gibbs’ Inequality）是信息论中的一个基本不等式，由 J.W. Gibbs 提出的，表明联合熵（Joint Entropy）不会超过两个随机变量各自熵（Entropy）的和，数学定义是：</p>
<p>对于任意两个随机变量 $X$ 和 $Y$，$H(X, Y) \leq H(X) + H(Y) $</p>
<p>当且仅当随机变量 $X$ 和 $Y$ 相互独立时，等号成立，即 $ H(X, Y) &#x3D; H(X) + H(Y) $。这是因为如果 $X$ 和 $Y $ 独立，那么知道 $X$ 的信息不会对 $Y$ 的不确定性产生任何影响，反之亦然。</p>
<h2 id="附录三：最大熵思想-Maximum-Entropy-Principle"><a href="#附录三：最大熵思想-Maximum-Entropy-Principle" class="headerlink" title="附录三：最大熵思想 (Maximum Entropy Principle)"></a>附录三：最大熵思想 (Maximum Entropy Principle)</h2><p>最大熵的思想是，当你要猜一个概率分布时，如果你对这个分布一无所知，那就猜熵最大的均匀分布；如果你对这个分布知道一些情况，那么，就猜满足这些情况下的熵最大的分布。<br>换句话说，除了已经确认的信息，不做任何未知假设，直接把所有未知事件当成<strong>等概率</strong>事件处理。</p>
<p>吴军老师的《数学之美》第 20 章中提到了一个掷骰子的例子可以很好的解释最大熵原理。对于一个骰子，每面向上的概率是多少，可能我们会不加思索会说是1&#x2F;6，但是，如果说骰子的其中四点被做过特殊处理，四点向上的概率为1&#x2F;3，那么其他点向上的概率则变为2&#x2F;15。</p>
<p>首先，在骰子没做任何处理之前，我们认为骰子的各个面出现的概率是相同的，即符合均匀分布，此时熵最大。然后，当增加四点被做过特殊处理后，其他面的向上的概率变为2&#x2F;15，在这里四点被做过特殊处理，即所谓的约束条件，满足约束条件之后，而对其它则不做任何假设其他面向上的概率是相同的，即为2&#x2F;15，也是熵最大的。</p>
<p>在这个例子中，不作任何假设就是使用“等概率”，这个时候概率分布最均匀，从而使得概率分布的熵最大，即最大熵原理。</p>
<h2 id="附录四：热力学第二定律的解释"><a href="#附录四：热力学第二定律的解释" class="headerlink" title="附录四：热力学第二定律的解释"></a>附录四：热力学第二定律的解释</h2><p>有一种说法，宇宙诞生时是符合帕累托分布的，它的熵值很低，也具有很大的活力；宇宙热寂时，它的熵值很大，符合正态分布，也就是说事物的发展过程就是从幂律分布到正态分布。</p>
<p><img src="/2024/07/16/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80-%E4%BF%A1%E6%81%AF%E7%86%B5/demo.png" alt="alt text"></p>
<p>社会学还有一个叫做二八定律的东西，20% 的人掌握着社会 80% 的财富，把这种离散的模型迁移到连续的情况，这就是帕累托分布（这是一个厚尾模型，它的均值在采样无限时可以趋于无限大）。从其图像可以看到，采样时大部分样本都聚集在后面，但采样足够多时，总有几个非常大的样本来影响采样数据整体的分布，这个概率分布的熵值就很低。<br>马克思主义指出，资本主义必将演化到共产主义，也就是大部分财富在少部分人手中的幂等分布，最终必将演化到正态分布，即大部分财富掌握在大部分人手中！</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a target="_blank" rel="noopener" href="https://faculty.ustc.edu.cn/_resources/group1/M00/00/37/wKhJFGN1l3WAH_v2AxmG8aJYbXg890.pdf">信息论基础 - Thomas M.Cover, Joy A,Thomas</a></li>
<li><a target="_blank" rel="noopener" href="https://zodiac911.github.io/blog/entropy-and-crossentropy.html#%E4%BA%A4%E5%8F%89%E7%86%B5-Cross-Entropy">通俗理解熵与交叉熵</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Blackteaxx/p/18164220">Entropy</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/@eric_zhu/%E7%86%B5-entropy-8299ccce0cf2">熵 (Entropy)</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/71bd778dfb5a">信息论基础（熵，互信息，交叉熵）- 老羊肖恩</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ZihanZhang/p/16049215.html">机器学习里的信息论</a></li>
<li><a target="_blank" rel="noopener" href="https://lumingdong.cn/various-entropies-in-machine-learning.html">机器学习中的各种熵</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/05/26/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/" rel="prev" title="CNN 卷积神经网络简介">
                  <i class="fa fa-angle-left"></i> CNN 卷积神经网络简介
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/07/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80%EF%BC%9A%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/" rel="next" title="大模型学习笔记之一：基本知识">
                  大模型学习笔记之一：基本知识 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Alex Sun</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">278k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">16:51</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/nokiam9/nokiam9.github.io.git" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
